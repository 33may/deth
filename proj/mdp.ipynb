{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neighbour suspect you', 'You have Food']\n",
      "Initial State: You dont have Food\n",
      "Available Actions: ['Buy Food', 'Take neighbour Food']\n",
      "Chose action: Buy Food\n",
      "Slipped\n",
      "Action Taken: Buy Food, New State: You have Food, Reward: -2, Finish: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, states, terminal_states, transitions, current_state=None, slippery_factor = 0.8, is_slippery = False, cost_of_living = 0.01 ):\n",
    "        self.states = states\n",
    "        self.terminal_states = terminal_states\n",
    "        self.actions = {state: list(action) for state, action in transitions.items()}\n",
    "        self.transitions = transitions\n",
    "        self.observation_space = len(states)\n",
    "        self.action_space = len(self.actions)\n",
    "        self.is_slippery = is_slippery\n",
    "        self.slippery_factor = slippery_factor\n",
    "        self.cost_of_living = cost_of_living\n",
    "        if current_state is None:\n",
    "            self.current_state = random.choice([s for s in states if s not in self.terminal_states])\n",
    "        else:\n",
    "            self.current_state = current_state\n",
    "\n",
    "    def reset(self):\n",
    "        available_states = [state for state in self.states if state not in self.terminal_states]\n",
    "        self.current_state = random.choice(available_states)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        random_number_generator = np.random.default_rng()\n",
    "        if self.current_state in self.terminal_states:\n",
    "            raise Exception(\"Already in a terminal state\")\n",
    "        if action not in self.get_available_actions():\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        if self.is_slippery and random_number_generator.random() < self.slippery_factor:\n",
    "            action = random.choice(self.get_available_actions())\n",
    "            print(f\"Slipped\")\n",
    "\n",
    "        outcomes = self.transitions[self.current_state][action]\n",
    "        \n",
    "        if not outcomes:\n",
    "            print(f\"No transitions available from this state({self.current_state}).\")\n",
    "            self.current_state = None  \n",
    "            return self.current_state, 0, True\n",
    "\n",
    "        possible_states = list(outcomes.keys())\n",
    "        probabilities = [outcomes[state][0] for state in possible_states]\n",
    "\n",
    "        next_state = random.choices(possible_states, weights=probabilities)[0]\n",
    "       \n",
    "        reward = outcomes[next_state][1]\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        done = self.current_state in self.terminal_states or not self.get_available_actions()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        return self.actions[self.current_state]\n",
    "\n",
    "    def get_possible_next_states(self):\n",
    "        possible_states = set()\n",
    "        for action in self.actions[self.current_state]:\n",
    "            outcomes = self.transitions[self.current_state][action].keys()\n",
    "            possible_states.update(outcomes)\n",
    "        return list(possible_states)\n",
    "\n",
    "\n",
    "\n",
    "states1_2 = [\n",
    "    'You have Food',\n",
    "    'You dont have Food',\n",
    "    'Neighbour suspect you'\n",
    "]\n",
    "\n",
    "terminal_states1_2 = [\n",
    "    'You have Food'\n",
    "]\n",
    "\n",
    "transitions1_2 = {\n",
    "    'You have Food': {\n",
    "        'Eat own food': {'You dont have Food': [1.0, 1]},\n",
    "        'Take neighbour Food': {'Neighbour suspect you': [0.2, -1], 'You have Food': [0.8, 1]}\n",
    "    },\n",
    "    'You dont have Food': {\n",
    "        'Buy Food': {'You have Food': [1.0, -2]},\n",
    "        'Take neighbour Food': {'Neighbour suspect you': [0.5, -1], 'You have Food': [0.5, 1]}\n",
    "    },\n",
    "    'Neighbour suspect you': {\n",
    "        'Buy Food': {'You have Food': [1.0, -2]},\n",
    "        'Take neighbour Food': {'Neighbour suspect you': [0.9, -5], 'You have Food': [0.1, 1]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "mdp1_2 = MDP(states1_2,terminal_states1_2, transitions1_2, is_slippery=True, slippery_factor=0.9)\n",
    "current_state = mdp1_2.reset()\n",
    "\n",
    "print(mdp1_2.get_possible_next_states())\n",
    "\n",
    "print(\"Initial State:\", current_state)\n",
    "\n",
    "available_actions = mdp1_2.get_available_actions()\n",
    "\n",
    "print(\"Available Actions:\", available_actions)\n",
    "\n",
    "action_to_take = available_actions[random.randint(0, 1)]\n",
    "\n",
    "print(\"Chose action:\", action_to_take)\n",
    "\n",
    "new_state, reward, done = mdp1_2.step(action_to_take)\n",
    "print(f\"Action Taken: {action_to_take}, New State: {new_state}, Reward: {reward}, Finish: {done}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:29:13.276418Z",
     "start_time": "2024-05-14T13:29:13.028905Z"
    }
   },
   "id": "2b0aad2f577a93a3",
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbour suspect you -> Take neighbour Food -> You have Food | Reward: 1\n",
      "Reached a terminal state.\n"
     ]
    }
   ],
   "source": [
    "mdp1_2.reset()\n",
    "\n",
    "while True:\n",
    "    current_state = mdp1_2.current_state\n",
    "    available_actions = mdp1_2.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    new_state, reward, done = mdp1_2.step(action)\n",
    "\n",
    "    print(f\"{current_state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:27:54.911109Z",
     "start_time": "2024-05-14T13:27:54.901878Z"
    }
   },
   "id": "ebe421ed42ff2e93",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 -> a0 -> S0 | Reward: 0\n",
      "S0 -> a0 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a0 -> S0 | Reward: 5\n",
      "S0 -> a0 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n"
     ]
    }
   ],
   "source": [
    "states1_3 = [\n",
    "    'S0',\n",
    "    'S1',\n",
    "    'S2'\n",
    "]\n",
    "\n",
    "transitions1_3 = {\n",
    "    'S0': {\n",
    "        'a0': {'S0': [0.5, 0], 'S2': [0.5, 0]},\n",
    "        'a1': {'S2': [1, 0]}\n",
    "    },\n",
    "    'S1': {\n",
    "        'a0': {'S0': [0.7, 5], 'S2': [0.2, 0], 'S1': [0.1, 0]},\n",
    "        'a1': {'S1': [0.95, 0], 'S2': [0.05, 0]}\n",
    "    },\n",
    "    'S2': {\n",
    "        'a1': {'S0': [0.3, -1], 'S2': [0.4, 0], 'S1': [0.3, 0]},\n",
    "        'a0': {'S0': [0.4, 0], 'S2': [0.6, 0]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "mdp1_3 = MDP(states1_3, [], transitions1_3)\n",
    "mdp1_3.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    current_state = mdp1_3.current_state\n",
    "    available_actions = mdp1_3.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    new_state, reward, done = mdp1_3.step(action)\n",
    "\n",
    "    print(f\"{current_state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T11:01:19.245143Z",
     "start_time": "2024-05-14T11:01:19.230035Z"
    }
   },
   "id": "63ba963d7cbfc1a7",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -> l -> 2 | Reward: 0\n",
      "2 -> r -> 3 | Reward: 0\n",
      "3 -> l -> 2 | Reward: 0\n",
      "2 -> l -> 1 | Reward: -1\n",
      "Reached a terminal state.\n"
     ]
    }
   ],
   "source": [
    "states2_1 = [\n",
    "    '1','2','3','4','5'\n",
    "]\n",
    "\n",
    "terminal_states2_1 = ['1','5']\n",
    "\n",
    "transitions2_1 = {\n",
    "    '1' : {\n",
    "        'r' : {'2' : [1, 0]}\n",
    "    },\n",
    "    '2' : {\n",
    "        'l' : {'1' : [1, -1]},\n",
    "        'r' : {'3' : [1, 0]}\n",
    "    },\n",
    "    '3' : {\n",
    "        'l' : {'2' : [1, 0]},\n",
    "        'r' : {'4' : [1, 0]}\n",
    "    },\n",
    "    '4' : {\n",
    "        'l' : {'3' : [1, 0]},\n",
    "        'r' : {'5' : [1, 1]}\n",
    "    },\n",
    "    '5' : {\n",
    "        'l' : {'4' : [1, 0]}\n",
    "    }\n",
    "}\n",
    "\n",
    "mdp2_1 = MDP(states2_1, terminal_states2_1, transitions2_1, slippery_factor=0.2, is_slippery=True, cost_of_living=0.1)\n",
    "\n",
    "mdp2_1.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    current_state = mdp2_1.current_state\n",
    "    available_actions = mdp2_1.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    new_state, reward, done = mdp2_1.step(action)\n",
    "\n",
    "    print(f\"{current_state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:00:32.683015Z",
     "start_time": "2024-05-14T13:00:32.666981Z"
    }
   },
   "id": "d11336c7d219e05a",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a0 -> S0 | Reward: 5\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "Q-Agent Total Rewards over 1000 episodes: 9934\n",
      "Random Agent Total Rewards over 1000 episodes: 4780\n"
     ]
    }
   ],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.q_table = {state: {action: 0 for action in mdp.actions[state]} for state in mdp.states}\n",
    "\n",
    "    def train(self,\n",
    "              episodes=400,\n",
    "              learning_rate=0.1,\n",
    "              discount_factor=0.9,\n",
    "              cost_of_living=0.01):\n",
    "\n",
    "        env = self.mdp\n",
    "        \n",
    "        self.q_table = {state: {action: 0 for action in env.actions[state]} for state in env.states}\n",
    "    \n",
    "        epsilon = 1\n",
    "        epsilon_decay = 1/(episodes * 0.9)\n",
    "        random_number_generator = np.random.default_rng()\n",
    "        rewards_per_episode = np.zeros(episodes)\n",
    "        time_rewards_per_episode = np.zeros(episodes)\n",
    "        steps_per_episode = []\n",
    "    \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "    \n",
    "            for step in range(20):\n",
    "                if random_number_generator.random() < epsilon:\n",
    "                    action = random.choice(env.get_available_actions())\n",
    "                else:\n",
    "                    action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "    \n",
    "                new_state, reward, terminated = env.step(action)\n",
    "    \n",
    "                # if terminated & (reward == 0):\n",
    "                #     reward = reward - 1\n",
    "\n",
    "\n",
    "                best_next_action = max(self.q_table[new_state], key=self.q_table[new_state].get)\n",
    "\n",
    "                target = reward + discount_factor * self.q_table[new_state][best_next_action]\n",
    "\n",
    "                td_error = target - self.q_table[state][action]\n",
    "                \n",
    "                self.q_table[state][action] += learning_rate * td_error\n",
    "    \n",
    "                state = new_state\n",
    "    \n",
    "                if terminated:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "            epsilon = max(epsilon - epsilon_decay, 0)\n",
    "    \n",
    "            if epsilon == 0:\n",
    "                learning_rate = learning_rate * 0.1\n",
    "\n",
    "        \n",
    "    def run(self, episodes = 1):\n",
    "\n",
    "        env = self.mdp\n",
    "        \n",
    "        total_reward = 0\n",
    "    \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "        \n",
    "            for i in range(20):\n",
    "                action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "        \n",
    "                new_state, reward, terminated = env.step(action)\n",
    "                \n",
    "                total_reward += reward\n",
    "                \n",
    "                print(f\"{state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "                \n",
    "                state = new_state\n",
    "\n",
    "            return total_reward\n",
    "\n",
    "\n",
    "    def evaluate_QAgent(self, episodes = 1):\n",
    "        env = self.mdp\n",
    "        total_reward = 0\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "\n",
    "            for i in range(20):\n",
    "                action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "                new_state, reward, terminated = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = new_state\n",
    "\n",
    "            return total_reward   \n",
    "        \n",
    "\n",
    "    def run_random_agent(self, episodes=1):\n",
    "        total_reward = 0\n",
    "        for _ in range(episodes):\n",
    "            state = self.mdp.reset()\n",
    "\n",
    "            for i in range(20):\n",
    "                action = random.choice(list(self.mdp.actions[state]))\n",
    "                state, reward, terminated = self.mdp.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "        return total_reward\n",
    "\n",
    "\n",
    "    def compare_agents(self, episodes=100):\n",
    "        q_agent_rewards = 0\n",
    "        random_agent_rewards = 0\n",
    "        for _ in range(episodes):\n",
    "            q_agent_rewards += self.evaluate_QAgent(1)\n",
    "            random_agent_rewards += self.run_random_agent(1)\n",
    "\n",
    "        print(f\"Q-Agent Total Rewards over {episodes} episodes: {q_agent_rewards}\")\n",
    "        print(f\"Random Agent Total Rewards over {episodes} episodes: {random_agent_rewards}\")\n",
    "    \n",
    "    \n",
    "agent = QAgent(mdp1_3)\n",
    "\n",
    "agent.train(episodes=1000)\n",
    "\n",
    "agent.run(episodes=1)\n",
    "\n",
    "agent.compare_agents(episodes=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:00:13.063333Z",
     "start_time": "2024-05-14T13:00:12.935948Z"
    }
   },
   "id": "2d3ef95855098bc9",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T11:19:23.631761Z",
     "start_time": "2024-05-14T11:19:23.628441Z"
    }
   },
   "id": "bd21079a15aec613",
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
