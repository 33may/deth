{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action: r\n",
      "2 -> r -> 3 | Reward: 0\n",
      "Chosen action: r\n",
      "3 -> r -> 4 | Reward: 0\n",
      "Chosen action: r\n",
      "Slipped\n",
      "4 -> r -> 5 | Reward: 1\n",
      "Reached a terminal state.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, states, terminal_states, transitions, current_state=None, slippery_factor = 0.8, is_slippery = False, cost_of_living = 0.01 ):\n",
    "        self.states = states\n",
    "        self.terminal_states = terminal_states\n",
    "        self.actions = {state: list(action) for state, action in transitions.items()}\n",
    "        self.transitions = transitions\n",
    "        self.observation_space = len(states)\n",
    "        self.action_space = len(self.actions)\n",
    "        self.is_slippery = is_slippery\n",
    "        self.slippery_factor = slippery_factor\n",
    "        self.cost_of_living = cost_of_living\n",
    "        if current_state is None:\n",
    "            self.current_state = random.choice([s for s in states if s not in self.terminal_states])\n",
    "        else:\n",
    "            self.current_state = current_state\n",
    "\n",
    "    def reset(self):\n",
    "        available_states = [state for state in self.states if state not in self.terminal_states]\n",
    "        self.current_state = random.choice(available_states)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        random_number_generator = np.random.default_rng()\n",
    "        if self.current_state in self.terminal_states:\n",
    "            raise Exception(\"Already in a terminal state\")\n",
    "        if action not in self.get_available_actions():\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        if self.is_slippery and random_number_generator.random() < self.slippery_factor:\n",
    "            action = random.choice(self.get_available_actions())\n",
    "            print(f\"Slipped\")\n",
    "\n",
    "        outcomes = self.transitions[self.current_state][action]\n",
    "        \n",
    "        if not outcomes:\n",
    "            print(f\"No transitions available from this state({self.current_state}).\")\n",
    "            self.current_state = None  \n",
    "            return self.current_state, 0, True\n",
    "\n",
    "        possible_states = list(outcomes.keys())\n",
    "        probabilities = [outcomes[state][0] for state in possible_states]\n",
    "\n",
    "        next_state = random.choices(possible_states, weights=probabilities)[0]\n",
    "        \n",
    "       \n",
    "        reward = outcomes[next_state][1]\n",
    "\n",
    "        print(f\"{current_state} -> {action} -> {next_state} | Reward: {reward}\")\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        done = self.current_state in self.terminal_states or not self.get_available_actions()\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def get_available_actions(self):\n",
    "        return self.actions[self.current_state]\n",
    "\n",
    "    def get_possible_next_states(self):\n",
    "        possible_states = set()\n",
    "        for action in self.actions[self.current_state]:\n",
    "            outcomes = self.transitions[self.current_state][action].keys()\n",
    "            possible_states.update(outcomes)\n",
    "        return list(possible_states)\n",
    "\n",
    "\n",
    "\n",
    "states2_1 = [\n",
    "    '1','2','3','4','5'\n",
    "]\n",
    "\n",
    "terminal_states2_1 = ['1','5']\n",
    "\n",
    "# transitions2_1 = {\n",
    "#     '1' : {\n",
    "#         'r' : {'2' : [1, 0]}\n",
    "#     },\n",
    "#     '2' : {\n",
    "#         'l' : {'1' : [1, -1]},\n",
    "#         'r' : {'3' : [1, 0]}\n",
    "#     },\n",
    "#     '3' : {\n",
    "#         'l' : {'2' : [1, 0]},\n",
    "#         'r' : {'4' : [1, 0]}\n",
    "#     },\n",
    "#     '4' : {\n",
    "#         'l' : {'3' : [1, 0]},\n",
    "#         'r' : {'5' : [1, 1]}\n",
    "#     },\n",
    "#     '5' : {\n",
    "#         'l' : {'4' : [1, 0]}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def create_transitions(num_states):\n",
    "    transitions = {}\n",
    "    for state in range(1, num_states + 1):\n",
    "        state_str = str(state)\n",
    "        transitions[state_str] = {}\n",
    "        if state < num_states:\n",
    "            transitions[state_str]['r'] = {str(state + 1): [1, 1 if state == num_states - 1 else 0]}\n",
    "        if state > 1:\n",
    "            transitions[state_str]['l'] = {str(state - 1): [1, -1 if state == 2 else 0]}\n",
    "    return transitions\n",
    "\n",
    "num_states = 5\n",
    "transitions2_1 = create_transitions(num_states)\n",
    "\n",
    "mdp2_1 = MDP(states2_1, terminal_states2_1, transitions2_1, slippery_factor=0.5, is_slippery=True, cost_of_living=0.1)\n",
    "\n",
    "mdp2_1.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    current_state = mdp2_1.current_state\n",
    "    available_actions = mdp2_1.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    print(f\"Chosen action:\", action)\n",
    "    new_state, reward, done = mdp2_1.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T14:32:26.769787Z",
     "start_time": "2024-05-14T14:32:26.752096Z"
    }
   },
   "id": "2b0aad2f577a93a3",
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbour suspect you -> Take neighbour Food -> You have Food | Reward: 1\n",
      "Reached a terminal state.\n"
     ]
    }
   ],
   "source": [
    "mdp1_2.reset()\n",
    "\n",
    "while True:\n",
    "    current_state = mdp1_2.current_state\n",
    "    available_actions = mdp1_2.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    new_state, reward, done = mdp1_2.step(action)\n",
    "\n",
    "    print(f\"{current_state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:27:54.911109Z",
     "start_time": "2024-05-14T13:27:54.901878Z"
    }
   },
   "id": "ebe421ed42ff2e93",
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S2 -> a0 -> S0 | Reward: 0\n",
      "S0 -> a0 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a0 -> S0 | Reward: 5\n",
      "S0 -> a0 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n"
     ]
    }
   ],
   "source": [
    "states1_3 = [\n",
    "    'S0',\n",
    "    'S1',\n",
    "    'S2'\n",
    "]\n",
    "\n",
    "transitions1_3 = {\n",
    "    'S0': {\n",
    "        'a0': {'S0': [0.5, 0], 'S2': [0.5, 0]},\n",
    "        'a1': {'S2': [1, 0]}\n",
    "    },\n",
    "    'S1': {\n",
    "        'a0': {'S0': [0.7, 5], 'S2': [0.2, 0], 'S1': [0.1, 0]},\n",
    "        'a1': {'S1': [0.95, 0], 'S2': [0.05, 0]}\n",
    "    },\n",
    "    'S2': {\n",
    "        'a1': {'S0': [0.3, -1], 'S2': [0.4, 0], 'S1': [0.3, 0]},\n",
    "        'a0': {'S0': [0.4, 0], 'S2': [0.6, 0]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "mdp1_3 = MDP(states1_3, [], transitions1_3)\n",
    "mdp1_3.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    current_state = mdp1_3.current_state\n",
    "    available_actions = mdp1_3.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    new_state, reward, done = mdp1_3.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T11:01:19.245143Z",
     "start_time": "2024-05-14T11:01:19.230035Z"
    }
   },
   "id": "63ba963d7cbfc1a7",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[161], line 42\u001B[0m\n\u001B[1;32m     39\u001B[0m num_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m\n\u001B[1;32m     40\u001B[0m transitions2_1 \u001B[38;5;241m=\u001B[39m create_transitions(num_states)\n\u001B[0;32m---> 42\u001B[0m mdp2_1 \u001B[38;5;241m=\u001B[39m MDP(states2_1, terminal_states2_1, transitions2_1, slippery_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m, is_slippery\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, cost_of_living\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.1\u001B[39m)\n\u001B[1;32m     44\u001B[0m mdp2_1\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_311_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_311_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/DataSpell.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread)\n",
      "File \u001B[0;32m/Applications/DataSpell.app/Contents/plugins/python-ce/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "states2_1 = [\n",
    "    '1','2','3','4','5'\n",
    "]\n",
    "\n",
    "terminal_states2_1 = ['1','5']\n",
    "\n",
    "# transitions2_1 = {\n",
    "#     '1' : {\n",
    "#         'r' : {'2' : [1, 0]}\n",
    "#     },\n",
    "#     '2' : {\n",
    "#         'l' : {'1' : [1, -1]},\n",
    "#         'r' : {'3' : [1, 0]}\n",
    "#     },\n",
    "#     '3' : {\n",
    "#         'l' : {'2' : [1, 0]},\n",
    "#         'r' : {'4' : [1, 0]}\n",
    "#     },\n",
    "#     '4' : {\n",
    "#         'l' : {'3' : [1, 0]},\n",
    "#         'r' : {'5' : [1, 1]}\n",
    "#     },\n",
    "#     '5' : {\n",
    "#         'l' : {'4' : [1, 0]}\n",
    "#     }\n",
    "# }\n",
    "\n",
    "def create_transitions(num_states):\n",
    "    transitions = {}\n",
    "    for state in range(1, num_states + 1):\n",
    "        state_str = str(state)\n",
    "        transitions[state_str] = {}\n",
    "        if state < num_states:\n",
    "            transitions[state_str]['r'] = {str(state + 1): [1, 1 if state == num_states - 1 else 0]}\n",
    "        if state > 1:\n",
    "            transitions[state_str]['l'] = {str(state - 1): [1, -1 if state == 2 else 0]}\n",
    "    return transitions\n",
    "\n",
    "num_states = 5\n",
    "transitions2_1 = create_transitions(num_states)\n",
    "\n",
    "mdp2_1 = MDP(states2_1, terminal_states2_1, transitions2_1, slippery_factor=0.5, is_slippery=True, cost_of_living=0.1)\n",
    "\n",
    "mdp2_1.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    current_state = mdp2_1.current_state\n",
    "    available_actions = mdp2_1.get_available_actions()\n",
    "    action = random.choice(available_actions)\n",
    "    print(f\"Chosen action:\", action)\n",
    "    new_state, reward, done = mdp2_1.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Reached a terminal state.\")\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T14:28:01.126432Z",
     "start_time": "2024-05-14T14:27:46.392837Z"
    }
   },
   "id": "d11336c7d219e05a",
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S1 | Reward: 0\n",
      "S1 -> a0 -> S0 | Reward: 5\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "S0 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S2 | Reward: 0\n",
      "S2 -> a1 -> S0 | Reward: -1\n",
      "Q-Agent Total Rewards over 1000 episodes: 9934\n",
      "Random Agent Total Rewards over 1000 episodes: 4780\n"
     ]
    }
   ],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, mdp):\n",
    "        self.mdp = mdp\n",
    "        self.q_table = {state: {action: 0 for action in mdp.actions[state]} for state in mdp.states}\n",
    "\n",
    "    def train(self,\n",
    "              episodes=400,\n",
    "              learning_rate=0.1,\n",
    "              discount_factor=0.9,\n",
    "              cost_of_living=0.01):\n",
    "\n",
    "        env = self.mdp\n",
    "        \n",
    "        self.q_table = {state: {action: 0 for action in env.actions[state]} for state in env.states}\n",
    "    \n",
    "        epsilon = 1\n",
    "        epsilon_decay = 1/(episodes * 0.9)\n",
    "        random_number_generator = np.random.default_rng()\n",
    "        rewards_per_episode = np.zeros(episodes)\n",
    "        time_rewards_per_episode = np.zeros(episodes)\n",
    "        steps_per_episode = []\n",
    "    \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "    \n",
    "            for step in range(20):\n",
    "                if random_number_generator.random() < epsilon:\n",
    "                    action = random.choice(env.get_available_actions())\n",
    "                else:\n",
    "                    action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "    \n",
    "                new_state, reward, terminated = env.step(action)\n",
    "    \n",
    "                # if terminated & (reward == 0):\n",
    "                #     reward = reward - 1\n",
    "\n",
    "\n",
    "                best_next_action = max(self.q_table[new_state], key=self.q_table[new_state].get)\n",
    "\n",
    "                target = reward + discount_factor * self.q_table[new_state][best_next_action]\n",
    "\n",
    "                td_error = target - self.q_table[state][action]\n",
    "                \n",
    "                self.q_table[state][action] += learning_rate * td_error\n",
    "    \n",
    "                state = new_state\n",
    "    \n",
    "                if terminated:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "            epsilon = max(epsilon - epsilon_decay, 0)\n",
    "    \n",
    "            if epsilon == 0:\n",
    "                learning_rate = learning_rate * 0.1\n",
    "\n",
    "        \n",
    "    def run(self, episodes = 1):\n",
    "\n",
    "        env = self.mdp\n",
    "        \n",
    "        total_reward = 0\n",
    "    \n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "        \n",
    "            for i in range(20):\n",
    "                action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "        \n",
    "                new_state, reward, terminated = env.step(action)\n",
    "                \n",
    "                total_reward += reward\n",
    "                \n",
    "                print(f\"{state} -> {action} -> {new_state} | Reward: {reward}\")\n",
    "                \n",
    "                state = new_state\n",
    "\n",
    "            return total_reward\n",
    "\n",
    "\n",
    "    def evaluate_QAgent(self, episodes = 1):\n",
    "        env = self.mdp\n",
    "        total_reward = 0\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()\n",
    "\n",
    "            for i in range(20):\n",
    "                action = max(self.q_table[state], key=self.q_table[state].get)\n",
    "                new_state, reward, terminated = env.step(action)\n",
    "                total_reward += reward\n",
    "                state = new_state\n",
    "\n",
    "            return total_reward   \n",
    "        \n",
    "\n",
    "    def run_random_agent(self, episodes=1):\n",
    "        total_reward = 0\n",
    "        for _ in range(episodes):\n",
    "            state = self.mdp.reset()\n",
    "\n",
    "            for i in range(20):\n",
    "                action = random.choice(list(self.mdp.actions[state]))\n",
    "                state, reward, terminated = self.mdp.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "        return total_reward\n",
    "\n",
    "\n",
    "    def compare_agents(self, episodes=100):\n",
    "        q_agent_rewards = 0\n",
    "        random_agent_rewards = 0\n",
    "        for _ in range(episodes):\n",
    "            q_agent_rewards += self.evaluate_QAgent(1)\n",
    "            random_agent_rewards += self.run_random_agent(1)\n",
    "\n",
    "        print(f\"Q-Agent Total Rewards over {episodes} episodes: {q_agent_rewards}\")\n",
    "        print(f\"Random Agent Total Rewards over {episodes} episodes: {random_agent_rewards}\")\n",
    "    \n",
    "    \n",
    "agent = QAgent(mdp1_3)\n",
    "\n",
    "agent.train(episodes=1000)\n",
    "\n",
    "agent.run(episodes=1)\n",
    "\n",
    "agent.compare_agents(episodes=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T13:00:13.063333Z",
     "start_time": "2024-05-14T13:00:12.935948Z"
    }
   },
   "id": "2d3ef95855098bc9",
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-14T11:19:23.631761Z",
     "start_time": "2024-05-14T11:19:23.628441Z"
    }
   },
   "id": "bd21079a15aec613",
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
